---
title: "p8105_hw6_hz2770"
author: "Hao Zheng"
date: "11/30/2021"
output: github_document
---

```{r}
library(tidyverse)
library(modelr)
library(mgcv)

birthweight_data = read.csv("./data/birthweight.csv")
```

## Problem 1

First, clean the birthweight dataset.
```{r}
birthweight_data = 
  birthweight_data %>% 
  mutate(
    babysex = as.factor(babysex),
    babysex = fct_recode(babysex, "male" = "1", "female" = "2"),
    
    frace = as.factor(frace),
    frace = fct_recode(frace, "White" = "1", "Black" = "2", "Asian" = "3", "Puerto Rican" = "4", "Other" = "8"),
    
    malform = as.logical(malform),
    
    mrace = as.factor(mrace),
    mrace = fct_recode(mrace, "White" = "1", "Black" = "2", "Asian" = "3", "Puerto Rican" = "4")
  )

birthweight_data %>% 
  skimr::skim()
```

So there is no missing value in the dataset.

### Fit a model
Let's hypothesize that birthweight is related to mother's weight at delivery `delwt`.

First, make a plot to see if there is a potential linear relationship between `bwt` and `delwt`.

```{r}
birthweight_data %>% 
  ggplot(aes(x = delwt, y = bwt)) +
  geom_point() +
  labs(
      title = "Baby's birth weight against mother's weight at delivery",
      x = "mother’s weight at delivery (pounds)",
      y = "baby’s birth weight (grams)"
    )
```

However, as we can see from the above plot, there is no clear linear relationship between `bwt` and `delwt`.

Emmm, maybe we should try explore the relationship between birthweight and another variable. Let's try baby’s length at birth `blength`.

```{r}
birthweight_data %>% 
  ggplot(aes(x = blength , y = bwt)) +
  geom_point() +
  labs(
      title = "Baby's birth weight against baby’s length at birth",
      x = "baby’s length at birth (centimeteres)",
      y = "baby’s birth weight (grams)"
    )
```

So there might be a linear relationship between birth weight and birth length. Then fit a linear model with `bwt` as outcome, `blength` as the predictor.

```{r}
fit = lm(bwt ~ blength, data = birthweight_data)

fit %>% 
  broom::tidy() %>% 
  knitr::kable(digits = 3)

```

Since the p value is very small(around zero), so we can establish a linear relationship between these two variables.


Now plot the residual plots
```{r}
birthweight_data %>% 
  add_residuals(fit) %>% 
  add_predictions(fit) %>%  
  ggplot(aes(x = pred, y = resid)) +
  geom_point() +
  labs(
      title = "Residual plot",
      x = "Predictions",
      y = "Residuals"
    )
```

From the plot of predicted values against residuals, we can see most of the residuals are near 0, but there are also some extremely normal values when prediction is under 1000.

### Compare with two other models

```{r}
# use length at birth and gestational age as predictors
fit1 = lm(bwt ~ blength + gaweeks, data = birthweight_data)

fit1 %>% 
  broom::tidy() %>% 
  knitr::kable(digits = 3)

# use head circumference, length, sex, and all interactions (including the three-way interaction) between these
fit2 = lm(bwt ~ bhead + blength + babysex + bhead*blength + bhead*babysex + blength*babysex + bhead*blength*babysex, data = birthweight_data)

fit2 %>% 
  broom::tidy() %>% 
  knitr::kable(digits = 3)
```
According to the results, the fit1 model is quite significant, but the fit2 model has a big p value for the term: interaction between `bhead` and `blength`.

### Cross-validation
```{r}
cv_df = 
  crossv_mc(birthweight_data, 100) %>% 
  mutate(
    fit_mod = map(train, ~lm(bwt ~ blength, data = .x)),
    fit1_mod = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
    fit2_mod = map(train, ~lm(bwt ~ bhead + blength + babysex + bhead*blength + bhead*babysex + blength*babysex + bhead*blength*babysex, data = .x))
    ) %>% 
  mutate(
    rmse_fit = map2_dbl(fit_mod, test, ~rmse(model = .x, data = .y)),
    rmse_fit1 = map2_dbl(fit1_mod, test, ~rmse(model = .x, data = .y)),
    rmse_fit2 = map2_dbl(fit2_mod, test, ~rmse(model = .x, data = .y))
  )
```

### RMSE model
```{r}
cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + 
  geom_violin() +
  labs(
      title = "RMSE distribution of each model",
      x = "Model",
      y = "Root mean squared errors"
    )
```

Based on the RMSE distribution plot, we know that the fit2 model has the lowest RMSE, and the fit model has the highest RMSE, which means the fit2 model may be the most optimal among these three models.


## Problem 2

Now let's turn to the 2017 Central Park weather data. First, download the data.
```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())

```

### Bootstrapping

```{r}
boot_sample = function(df){
  sample_frac(df, size = 1, replace = TRUE)
}

boot_straps = 
  data_frame(
    strap_number = 1:5000,
    strap_sample = rerun(5000, boot_sample(weather_df))
  )

```

### Adjusted R_squared

```{r}
adj_r_squared = 
  boot_straps %>% 
  mutate(
    models = map(strap_sample, ~lm(tmax ~ tmin, data = .x)),
    results = map(models, broom::glance)
  ) %>% 
  select(strap_number, results) %>%
  unnest(results) %>%
  select(strap_number, adj.r.squared)

# make the distribution plot
adj_r_squared %>%
  ggplot(aes(x = adj.r.squared)) +
  geom_density() +
  labs(
    x = "adjusted r squared",
    title = "Adjusted R Squared distribution"
  )

# 95% CI
adj_r_squared_CI = 
  adj_r_squared %>% 
  summarise(
    ci_lower = quantile(adj.r.squared, 0.025), 
    ci_upper = quantile(adj.r.squared, 0.975)
  )

adj_r_squared_CI
```

The adjusted r squared follows an approximately normal distribution, unimodel, symmetric, with a little longer left tail and mean around 0.91. And the 95% confidence interval for the adjusted r squared value is `[0.894, 0.927]`.

### log(beta0 * beta1)

```{r}
log_beta = 
  boot_straps %>% 
  mutate(
    models = map(strap_sample, ~ lm(tmax ~ tmin, data = .x)),
    results = map(models, broom::tidy)
  ) %>% 
  select(strap_number, results) %>%
  unnest(results) %>%
  select(strap_number, term, estimate) %>% 
  pivot_wider(
    names_from = term,
    values_from = estimate
  ) %>% 
  janitor::clean_names() %>% 
  mutate(
    log_value = log(intercept * tmin) 
  )

# make the distribution plot
log_beta %>% 
  ggplot(aes(x = log_value)) +
  geom_density() +
  labs(
    x = "log(beta0 * beta1)",
    title = "log(beta0 * beta1) distribution"
  )

# 95% CI
log_beta_CI = 
  log_beta %>% 
  summarise(
    ci_lower = quantile(log_value, 0.025), 
    ci_upper = quantile(log_value, 0.975)
  )

log_beta_CI
```

The distribution is also approximately normal, unimodel, quite symmetric, with a mean around 2.02. The 95% confidence interval for `log(beta0 * beta1)` is `[1.96, 2.06]`.
